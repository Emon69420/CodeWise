[{"id": "Directory structure:_0", "file": "Directory structure:", "content": "\u2514\u2500\u2500 emon69420-signlanginter/\n    \u2514\u2500\u2500 signLangInt/\n        \u251c\u2500\u2500 app.py\n        \u251c\u2500\u2500 collectionData.py\n        \u251c\u2500\u2500 data.py\n        \u251c\u2500\u2500 function.py\n        \u251c\u2500\u2500 languageModel.py\n        \u251c\u2500\u2500 model.h5\n        \u251c\u2500\u2500 model.json\n        \u251c\u2500\u2500 trainmodel.py\n        \u2514\u2500\u2500 Logs/\n            \u2514\u2500\u2500 train/\n                \u2514\u2500\u2500 events.out.tfevents.1729579555.ASUS.28416.0.v2"}, {"id": "signLangInt/app.py_0", "file": "signLangInt/app.py", "content": "================================================\nfrom function import *\nfrom keras.utils import to_categorical\nfrom keras.models import model_from_json\nfrom keras.layers import LSTM, Dense\nfrom keras.callbacks import TensorBoard\njson_file = open(\"model.json\", \"r\")\nmodel_json = json_file.read()\njson_file.close()\nmodel = model_from_json(model_json)\nmodel.load_weights(\"model.h5\")\n\ncolors = []\nfor i in range(0,20):\n    colors.append((245,117,16))\nprint(len(colors))"}, {"id": "signLangInt/app.py_1", "file": "signLangInt/app.py", "content": "def prob_viz(res, actions, input_frame, colors,threshold):\n    output_frame = input_frame.copy()\n    for num, prob in enumerate(res):\n        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n        \n    return output_frame\n\n\n# 1. New detection variables\nsequence = []\nsentence = []\naccuracy=[]\npredictions = []\nthreshold = 0.8 \n\ncap = cv2.VideoCapture(0)\n# cap = cv2.VideoCapture(\"https://192.168.43.41:8080/video\")\n# Set mediapipe model \nwith mp_hands.Hands(\n    model_complexity=0,\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as hands:\n    while cap.isOpened():\n\n        # Read feed"}, {"id": "signLangInt/app.py_2", "file": "signLangInt/app.py", "content": "min_tracking_confidence=0.5) as hands:\n    while cap.isOpened():\n\n        # Read feed\n        ret, frame = cap.read()\n\n        # Make detections\n        cropframe=frame[40:400,0:300]\n        # print(frame.shape)\n        frame=cv2.rectangle(frame,(0,40),(300,400),255,2)\n        # frame=cv2.putText(frame,\"Active Region\",(75,25),cv2.FONT_HERSHEY_COMPLEX_SMALL,2,255,2)\n        image, results = mediapipe_detection(cropframe, hands)\n        # print(results)\n        \n        # Draw landmarks\n        # draw_styled_landmarks(image, results)\n        # 2. Prediction logic\n        keypoints = extract_keypoints(results)\n        sequence.append(keypoints)\n        sequence = sequence[-30:]\n\n        try: \n            if len(sequence) == 30:"}, {"id": "signLangInt/app.py_3", "file": "signLangInt/app.py", "content": "sequence = sequence[-30:]\n\n        try: \n            if len(sequence) == 30:\n                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n                print(actions[np.argmax(res)])\n                predictions.append(np.argmax(res))\n                \n                \n            #3. Viz logic\n                if np.unique(predictions[-10:])[0]==np.argmax(res): \n                    if res[np.argmax(res)] > threshold: \n                        if len(sentence) > 0: \n                            if actions[np.argmax(res)] != sentence[-1]:\n                                sentence.append(actions[np.argmax(res)])\n                                accuracy.append(str(res[np.argmax(res)]*100))\n                        else:"}, {"id": "signLangInt/app.py_4", "file": "signLangInt/app.py", "content": "else:\n                            sentence.append(actions[np.argmax(res)])\n                            accuracy.append(str(res[np.argmax(res)]*100)) \n\n                if len(sentence) > 1: \n                    sentence = sentence[-1:]\n                    accuracy=accuracy[-1:]\n\n                # Viz probabilities\n                # frame = prob_viz(res, actions, frame, colors,threshold)\n        except Exception as e:\n            # print(e)\n            pass\n            \n        cv2.rectangle(frame, (0,0), (300, 40), (245, 117, 16), -1)\n        cv2.putText(frame,\"Output: -\"+' '.join(sentence)+''.join(accuracy), (3,30), \n                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n        \n        # Show to screen"}, {"id": "signLangInt/app.py_5", "file": "signLangInt/app.py", "content": "# Show to screen\n        cv2.imshow('OpenCV Feed', frame)\n\n        # Break gracefully\n        if cv2.waitKey(10) & 0xFF == ord('q'):\n            break\n    cap.release()\n    cv2.destroyAllWindows()"}, {"id": "signLangInt/collectionData.py_0", "file": "signLangInt/collectionData.py", "content": "================================================\nimport os\nimport cv2\ncap=cv2.VideoCapture(0)\ndirectory='Image/'\nwhile True:\n    _,frame=cap.read()\n    count = {\n             'a': len(os.listdir(directory+\"/A\")),\n             'b': len(os.listdir(directory+\"/B\")),\n             'c': len(os.listdir(directory+\"/C\")),\n             'd': len(os.listdir(directory+\"/D\")),\n             'e': len(os.listdir(directory+\"/E\")),\n             'f': len(os.listdir(directory+\"/F\")),\n             'g': len(os.listdir(directory+\"/G\")),\n             'h': len(os.listdir(directory+\"/H\")),\n             'i': len(os.listdir(directory+\"/I\")),\n             'j': len(os.listdir(directory+\"/J\")),\n             'k': len(os.listdir(directory+\"/K\")),\n             'l': len(os.listdir(directory+\"/L\")),"}, {"id": "signLangInt/collectionData.py_1", "file": "signLangInt/collectionData.py", "content": "'l': len(os.listdir(directory+\"/L\")),\n             'm': len(os.listdir(directory+\"/M\")),\n             'n': len(os.listdir(directory+\"/N\")),\n             'o': len(os.listdir(directory+\"/O\")),\n             'p': len(os.listdir(directory+\"/P\")),\n             'q': len(os.listdir(directory+\"/Q\")),\n             'r': len(os.listdir(directory+\"/R\")),\n             's': len(os.listdir(directory+\"/S\")),\n             't': len(os.listdir(directory+\"/T\")),\n             'u': len(os.listdir(directory+\"/U\")),\n             'v': len(os.listdir(directory+\"/V\")),\n             'w': len(os.listdir(directory+\"/W\")),\n             'x': len(os.listdir(directory+\"/X\")),\n             'y': len(os.listdir(directory+\"/Y\")),\n             'z': len(os.listdir(directory+\"/Z\"))\n             }"}, {"id": "signLangInt/collectionData.py_2", "file": "signLangInt/collectionData.py", "content": "'z': len(os.listdir(directory+\"/Z\"))\n             }\n    # cv2.putText(frame, \"a : \"+str(count['a']), (10, 100), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"b : \"+str(count['b']), (10, 110), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"c : \"+str(count['c']), (10, 120), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"d : \"+str(count['d']), (10, 130), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"e : \"+str(count['e']), (10, 140), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"f : \"+str(count['f']), (10, 150), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"g : \"+str(count['g']), (10, 160), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)"}, {"id": "signLangInt/collectionData.py_3", "file": "signLangInt/collectionData.py", "content": "# cv2.putText(frame, \"h : \"+str(count['h']), (10, 170), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"i : \"+str(count['i']), (10, 180), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"k : \"+str(count['k']), (10, 190), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"l : \"+str(count['l']), (10, 200), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"m : \"+str(count['m']), (10, 210), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"n : \"+str(count['n']), (10, 220), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"o : \"+str(count['o']), (10, 230), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)"}, {"id": "signLangInt/collectionData.py_4", "file": "signLangInt/collectionData.py", "content": "# cv2.putText(frame, \"p : \"+str(count['p']), (10, 240), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"q : \"+str(count['q']), (10, 250), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"r : \"+str(count['r']), (10, 260), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"s : \"+str(count['s']), (10, 270), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"t : \"+str(count['t']), (10, 280), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"u : \"+str(count['u']), (10, 290), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"v : \"+str(count['v']), (10, 300), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)"}, {"id": "signLangInt/collectionData.py_5", "file": "signLangInt/collectionData.py", "content": "# cv2.putText(frame, \"w : \"+str(count['w']), (10, 310), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"x : \"+str(count['x']), (10, 320), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"y : \"+str(count['y']), (10, 330), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    # cv2.putText(frame, \"z : \"+str(count['z']), (10, 340), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,255), 1)\n    row = frame.shape[1]\n    col = frame.shape[0]\n    cv2.rectangle(frame,(0,40),(300,400),(255,255,255),2)\n    cv2.imshow(\"data\",frame)\n    cv2.imshow(\"ROI\",frame[40:400,0:300])\n    frame=frame[40:400,0:300]\n    interrupt = cv2.waitKey(10)\n    if interrupt & 0xFF == ord('a'):\n        cv2.imwrite(directory+'A/'+str(count['a'])+'.png',frame)\n    if interrupt & 0xFF == ord('b'):"}, {"id": "signLangInt/collectionData.py_6", "file": "signLangInt/collectionData.py", "content": "if interrupt & 0xFF == ord('b'):\n        cv2.imwrite(directory+'B/'+str(count['b'])+'.png',frame)\n    if interrupt & 0xFF == ord('c'):\n        cv2.imwrite(directory+'C/'+str(count['c'])+'.png',frame)\n    if interrupt & 0xFF == ord('d'):\n        cv2.imwrite(directory+'D/'+str(count['d'])+'.png',frame)\n    if interrupt & 0xFF == ord('e'):\n        cv2.imwrite(directory+'E/'+str(count['e'])+'.png',frame)\n    if interrupt & 0xFF == ord('f'):\n        cv2.imwrite(directory+'F/'+str(count['f'])+'.png',frame)\n    if interrupt & 0xFF == ord('g'):\n        cv2.imwrite(directory+'G/'+str(count['g'])+'.png',frame)\n    if interrupt & 0xFF == ord('h'):\n        cv2.imwrite(directory+'H/'+str(count['h'])+'.png',frame)\n    if interrupt & 0xFF == ord('i'):"}, {"id": "signLangInt/collectionData.py_7", "file": "signLangInt/collectionData.py", "content": "if interrupt & 0xFF == ord('i'):\n        cv2.imwrite(directory+'I/'+str(count['i'])+'.png',frame)\n    if interrupt & 0xFF == ord('j'):\n        cv2.imwrite(directory+'J/'+str(count['j'])+'.png',frame)\n    if interrupt & 0xFF == ord('k'):\n        cv2.imwrite(directory+'K/'+str(count['k'])+'.png',frame)\n    if interrupt & 0xFF == ord('l'):\n        cv2.imwrite(directory+'L/'+str(count['l'])+'.png',frame)\n    if interrupt & 0xFF == ord('m'):\n        cv2.imwrite(directory+'M/'+str(count['m'])+'.png',frame)\n    if interrupt & 0xFF == ord('n'):\n        cv2.imwrite(directory+'N/'+str(count['n'])+'.png',frame)\n    if interrupt & 0xFF == ord('o'):\n        cv2.imwrite(directory+'O/'+str(count['o'])+'.png',frame)\n    if interrupt & 0xFF == ord('p'):"}, {"id": "signLangInt/collectionData.py_8", "file": "signLangInt/collectionData.py", "content": "if interrupt & 0xFF == ord('p'):\n        cv2.imwrite(directory+'P/'+str(count['p'])+'.png',frame)\n    if interrupt & 0xFF == ord('q'):\n        cv2.imwrite(directory+'Q/'+str(count['q'])+'.png',frame)\n    if interrupt & 0xFF == ord('r'):\n        cv2.imwrite(directory+'R/'+str(count['r'])+'.png',frame)\n    if interrupt & 0xFF == ord('s'):\n        cv2.imwrite(directory+'S/'+str(count['s'])+'.png',frame)\n    if interrupt & 0xFF == ord('t'):\n        cv2.imwrite(directory+'T/'+str(count['t'])+'.png',frame)\n    if interrupt & 0xFF == ord('u'):\n        cv2.imwrite(directory+'U/'+str(count['u'])+'.png',frame)\n    if interrupt & 0xFF == ord('v'):\n        cv2.imwrite(directory+'V/'+str(count['v'])+'.png',frame)\n    if interrupt & 0xFF == ord('w'):"}, {"id": "signLangInt/collectionData.py_9", "file": "signLangInt/collectionData.py", "content": "if interrupt & 0xFF == ord('w'):\n        cv2.imwrite(directory+'W/'+str(count['w'])+'.png',frame)\n    if interrupt & 0xFF == ord('x'):\n        cv2.imwrite(directory+'X/'+str(count['x'])+'.png',frame)\n    if interrupt & 0xFF == ord('y'):\n        cv2.imwrite(directory+'Y/'+str(count['y'])+'.png',frame)\n    if interrupt & 0xFF == ord('z'):\n        cv2.imwrite(directory+'Z/'+str(count['z'])+'.png',frame)\n\n\ncap.release()\ncv2.destroyAllWindows()"}, {"id": "signLangInt/data.py_0", "file": "signLangInt/data.py", "content": "================================================\nfrom function import *\nfrom time import sleep\n\nfor action in actions: \n    for sequence in range(no_sequences):\n        try: \n            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n        except:\n            pass\n\n# cap = cv2.VideoCapture(0)\n# Set mediapipe model \nwith mp_hands.Hands(\n    model_complexity=0,\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as hands:\n    \n    # NEW LOOP\n    # Loop through actions\n    for action in actions:\n        # Loop through sequences aka videos\n        for sequence in range(no_sequences):\n            # Loop through video length aka sequence length\n            for frame_num in range(sequence_length):\n\n                # Read feed\n                # ret, frame = cap.read()"}, {"id": "signLangInt/data.py_1", "file": "signLangInt/data.py", "content": "# Read feed\n                # ret, frame = cap.read()\n                frame=cv2.imread('Image/{}/{}.png'.format(action,sequence))\n                # frame=cv2.imread('{}{}.png'.format(action,sequence))\n                # frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n\n                # Make detections\n                image, results = mediapipe_detection(frame, hands)\n#                 print(results)\n\n                # Draw landmarks\n                draw_styled_landmarks(image, results)\n                \n                # NEW Apply wait logic\n                if frame_num == 0:\n                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)"}, {"id": "signLangInt/data.py_2", "file": "signLangInt/data.py", "content": "cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n                    # Show to screen\n                    cv2.imshow('OpenCV Feed', image)\n                    cv2.waitKey(200)\n                else: \n                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n                    # Show to screen\n                    cv2.imshow('OpenCV Feed', image)"}, {"id": "signLangInt/data.py_3", "file": "signLangInt/data.py", "content": "cv2.imshow('OpenCV Feed', image)\n                \n                # NEW Export keypoints\n                keypoints = extract_keypoints(results)\n                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n                np.save(npy_path, keypoints)\n\n                # Break gracefully\n                if cv2.waitKey(10) & 0xFF == ord('q'):\n                    break\n                    \n    # cap.release()\n    cv2.destroyAllWindows()"}, {"id": "signLangInt/function.py_0", "file": "signLangInt/function.py", "content": "================================================\n#import dependency\nimport cv2\nimport numpy as np\nimport os\nimport mediapipe as mp\n\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_hands = mp.solutions.hands\n\ndef mediapipe_detection(image, model):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n    image.flags.writeable = False                  # Image is no longer writeable\n    results = model.process(image)                 # Make prediction\n    image.flags.writeable = True                   # Image is now writeable \n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n    return image, results"}, {"id": "signLangInt/function.py_1", "file": "signLangInt/function.py", "content": "def draw_styled_landmarks(image, results):\n    if results.multi_hand_landmarks:\n      for hand_landmarks in results.multi_hand_landmarks:\n        mp_drawing.draw_landmarks(\n            image,\n            hand_landmarks,\n            mp_hands.HAND_CONNECTIONS,\n            mp_drawing_styles.get_default_hand_landmarks_style(),\n            mp_drawing_styles.get_default_hand_connections_style())"}, {"id": "signLangInt/function.py_2", "file": "signLangInt/function.py", "content": "def extract_keypoints(results):\n    if results.multi_hand_landmarks:\n      for hand_landmarks in results.multi_hand_landmarks:\n        rh = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten() if hand_landmarks else np.zeros(21*3)\n        return(np.concatenate([rh]))\n# Path for exported data, numpy arrays\nDATA_PATH = os.path.join('MP_Data') \n\nactions = np.array([\"FUCK U\",\"D\", \"c\"])\n\nno_sequences = 50\n\nsequence_length = 50"}, {"id": "signLangInt/languageModel.py_0", "file": "signLangInt/languageModel.py", "content": "================================================\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nimport cv2\nimport numpy as np\nimport os\n\n# Load your preprocessed dataset (X for images, y for labels)\n# Replace 'path/to/your/dataset' with the actual path to your dataset\nX_train = np.load('path/to/your/dataset/X_train.npy')\ny_train = np.load('path/to/your/dataset/y_train.npy')\nX_val = np.load('path/to/your/dataset/X_val.npy')\ny_val = np.load('path/to/your/dataset/y_val.npy')\n\n# Create the model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),"}, {"id": "signLangInt/languageModel.py_1", "file": "signLangInt/languageModel.py", "content": "Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(num_classes, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n\n# Save the trained model\nmodel.save('sign_language_model.h5')\n\n# Visualize training history (optional)\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"}, {"id": "signLangInt/model.h5_0", "file": "signLangInt/model.h5", "content": "================================================\n[Binary file]"}, {"id": "signLangInt/model.json_0", "file": "signLangInt/model.json", "content": "================================================"}, {"id": "signLangInt/model.json_1", "file": "signLangInt/model.json", "content": "{\"module\": \"keras\", \"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"layers\": [{\"module\": \"keras.layers\", \"class_name\": \"InputLayer\", \"config\": {\"batch_shape\": [null, 30, 63], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"input_layer\"}, \"registered_name\": null}, {\"module\": \"keras.layers\", \"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"return_sequences\": true, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"zero_output_for_mask\": false, \"units\": 64, \"activation\": \"relu\","}, {"id": "signLangInt/model.json_2", "file": "signLangInt/model.json", "content": "false, \"unroll\": false, \"zero_output_for_mask\": false, \"units\": 64, \"activation\": \"relu\", \"recurrent_activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"recurrent_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"OrthogonalInitializer\", \"config\": {\"seed\": null, \"gain\": 1.0}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0,"}, {"id": "signLangInt/model.json_3", "file": "signLangInt/model.json", "content": "\"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"seed\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 30, 63]}}, {\"module\": \"keras.layers\", \"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_1\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"return_sequences\": true, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"zero_output_for_mask\": false, \"units\": 128, \"activation\": \"relu\", \"recurrent_activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null},"}, {"id": "signLangInt/model.json_4", "file": "signLangInt/model.json", "content": "\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"recurrent_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"OrthogonalInitializer\", \"config\": {\"seed\": null, \"gain\": 1.0}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"seed\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 30, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_2\","}, {"id": "signLangInt/model.json_5", "file": "signLangInt/model.json", "content": "[null, 30, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_2\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"return_sequences\": false, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"zero_output_for_mask\": false, \"units\": 64, \"activation\": \"relu\", \"recurrent_activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"recurrent_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"OrthogonalInitializer\", \"config\": {\"seed\": null, \"gain\": 1.0}, \"registered_name\": null}, \"bias_initializer\": {\"module\":"}, {"id": "signLangInt/model.json_6", "file": "signLangInt/model.json", "content": "\"config\": {\"seed\": null, \"gain\": 1.0}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"seed\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 30, 128]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 64, \"activation\": \"relu\", \"use_bias\":"}, {"id": "signLangInt/model.json_7", "file": "signLangInt/model.json", "content": "{\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 32, \"activation\": \"relu\", \"use_bias\": true,"}, {"id": "signLangInt/model.json_8", "file": "signLangInt/model.json", "content": "{\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 64]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 1, \"activation\": \"softmax\", \"use_bias\":"}, {"id": "signLangInt/model.json_9", "file": "signLangInt/model.json", "content": "{\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 1, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32]}}], \"build_input_shape\": [null, 30, 63]}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 30, 63]}, \"compile_config\": {\"optimizer\": {\"module\": \"keras.optimizers\", \"class_name\": \"Adam\", \"config\": {\"name\": \"adam\", \"learning_rate\": 0.0010000000474974513,"}, {"id": "signLangInt/model.json_10", "file": "signLangInt/model.json", "content": "\"class_name\": \"Adam\", \"config\": {\"name\": \"adam\", \"learning_rate\": 0.0010000000474974513, \"weight_decay\": null, \"clipnorm\": null, \"global_clipnorm\": null, \"clipvalue\": null, \"use_ema\": false, \"ema_momentum\": 0.99, \"ema_overwrite_frequency\": null, \"loss_scale_factor\": null, \"gradient_accumulation_steps\": null, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1e-07, \"amsgrad\": false}, \"registered_name\": null}, \"loss\": \"categorical_crossentropy\", \"loss_weights\": null, \"metrics\": [\"categorical_accuracy\"], \"weighted_metrics\": null, \"run_eagerly\": false, \"steps_per_execution\": 1, \"jit_compile\": false}}"}, {"id": "signLangInt/trainmodel.py_0", "file": "signLangInt/trainmodel.py", "content": "================================================\nfrom function import *\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom keras.callbacks import TensorBoard\nlabel_map = {label:num for num, label in enumerate(actions)}\n# print(label_map)\nsequences, labels = [], []\nfor action in actions:\n    for sequence in range(no_sequences):\n        window = []\n        for frame_num in range(sequence_length):\n            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n            window.append(res)\n        sequences.append(window)\n        labels.append(label_map[action])\n\nX = np.array(sequences)\ny = to_categorical(labels).astype(int)"}, {"id": "signLangInt/trainmodel.py_1", "file": "signLangInt/trainmodel.py", "content": "X = np.array(sequences)\ny = to_categorical(labels).astype(int)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n\nlog_dir = os.path.join('Logs')\ntb_callback = TensorBoard(log_dir=log_dir)\nmodel = Sequential()\nmodel.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(50,63)))\nmodel.add(LSTM(128, return_sequences=True, activation='relu'))\nmodel.add(LSTM(64, return_sequences=False, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(actions.shape[0], activation='softmax'))\nres = [.7, 0.2, 0.1]\n\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nmodel.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])\nmodel.summary()"}, {"id": "signLangInt/trainmodel.py_2", "file": "signLangInt/trainmodel.py", "content": "model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])\nmodel.summary()\n\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\nmodel.save('model.h5')"}, {"id": "signLangInt/Logs/train/events.out.tfevents.1729579555.ASUS.28416.0.v2_0", "file": "signLangInt/Logs/train/events.out.tfevents.1729579555.ASUS.28416.0.v2", "content": "================================================\n[Binary file]"}]